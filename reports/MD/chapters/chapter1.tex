\chapter{Искусственные нейронные сети и их применение в криптографии}
\label{c:}
\section{Обзор литературы}
В этом разделе будет выполнен аналитический обзор литературы по теме "--- анализ работ, выполненных ранее отечественными и зарубежными исследователями, описание имеющихся подходов к исследованию проблемы.
\bigskip

Впервые исследования по использованию нейронных сетей в криптографии были проведены в 1998 году в статье \cite{clark},
в которой была представлена криптографическая система на основе нейронных сетей. Другое применение нейронных сетей в криптографии
была опубликована Кинзелом и Кантером, которые представили способ использования нейронных
сети при обмене секретными ключами по общедоступному каналу \cite{kinzel}. Значительный вклад в использование нейронных сетей в криптографии
внес Ли в \cite{lis}, который предложил использовать нейронные сети для оптимизации дифференциального
и линейного криптоанализа. В 2009 году предложили использовать сигмоидальную функцию в качестве активационной функции в нейронной сети,
используемой при криптоанализе шифров Фейстеля \cite{rao}.
\bigskip

В 2012 была опубликовано работа \cite{neuro_des}, в которой нейронные сети успешно использовались для аппроксимации DES и Triple-DES.
В этой работе была использована модель представленная на рисунке \ref{model_1}.
Модель описывает атаку с открытым текстом. Эта атака основана на обучение нейронной сети, для последующего процесса расшифровки, не зная ключа используемого при шифровании.
\bigskip

В нейронную сеть подается зашифрованный текст в качестве входных данных, и открытый текст в качестве эталона.
После достаточного количества тренировок, с достаточным количеством пар открытого текста и шифротекста, который зашифрован неизменняем ключом,
предпологается, что нейронная сеть сможет расшифровать шифротекст, который не был представлен в обучающей выборке, при условии того, что он зашифрован на том же ключе, что и шифротекст из обучающей выборки.

\tikzset{%
	block/.style = {draw, thick, rectangle, minimum height = 3em,
		minimum width = 3em},
	sum/.style = {draw, circle, node distance = 2cm}, % Adder
	input/.style = {coordinate}, % Input
	output/.style = {coordinate} % Output
}

\begin{figure}[H]
	\begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]
	\draw
	
	node at (0,0)[right=-3mm]{\Large$\bullet$}
	node [input, name=plain] {}
	node at (0,-3)[right=-3mm]{\Large$\bullet$}
	node at (0,-3) [input, name=cipher] {}
	
	node at (5,-3)[block] (nn) {$Neural\ Network$}
	node at (10, 0)[block] (err) {$Error\ Function$}
	
	node at (10,-6)[block] (weight) {$Weights\ Correction$};
	
	\draw[->](plain) -- node {$Plaintext$} (err);
	\draw[->](cipher) -- node {$Ciphertext$} (nn);
	\draw[->](nn) -- node {} (err);
	
	\draw[->](err) -- node {} (weight);
	\draw[->](weight) -- node {} (nn);
	
	\end{tikzpicture}
	\caption{Модель криптоанализа на основе нейронной сети}
	\label{model_1}
\end{figure}

Исследования, которые использовали данную модель, показали высокий результат. Поэтому в нашем исследовании мы будем использовать схожую модель.
Так как процесс шифрования и расшифрования в сети Фейстеля симметричен, то наша модель криптоанализа будет использовать модель симметричную предыдущей. Данная модель представлена на рисунке \ref{model_2}.

\begin{figure}[H]
	\begin{tikzpicture}[auto, thick, node distance=2cm, >=triangle 45]
	\draw
	
	node at (0,0)[right=-3mm]{\Large$\bullet$}
	node [input, name=plain] {}
	node at (0,-3)[right=-3mm]{\Large$\bullet$}
	node at (0,-3) [input, name=cipher] {}
	
	node at (5,-3)[block] (nn) {$Neural\ Network$}
	node at (10, 0)[block] (err) {$Error\ Function$}
	
	node at (10,-6)[block] (weight) {$Weights\ Correction$};
	
	\draw[->](plain) -- node {$Ciphertext$} (err);
	\draw[->](cipher) -- node {$Plaintext$} (nn);
	\draw[->](nn) -- node {} (err);
	
	\draw[->](err) -- node {} (weight);
	\draw[->](weight) -- node {} (nn);
	
	\end{tikzpicture}
	\caption{Модель криптоанализа на основе нейронной сети}
	\label{model_2}
\end{figure}

\newpage
\section{Искусственные нейронные сети: архитектура, параметры, программные средства}
В данном разделе мы рассмотрим основные типы нейронных сетей, используемых в нейронной криптографии. А также в подразделе \ref{chapter1_section3} проведем исследования для определения наилучшей модели искусственной нейронной сети и ее параметров для аппроксимации криптографических преобразований.
\bigskip

Для проведения исследований по определению наилучшей модели искусственной нейронной сети определим задачи, которые должна решить искомая нейронная сеть:
\begin{enumerate}
	\item Ни одно криптографическое преобразование ни обходиться без таких бинарных операций как, циклический сдвиг вправо ($\ggg$), циклический сдвиг влево ($\lll$),
	операция побитового исключения ($\oplus$), операция сложения по модулю ($\boxplus$) и применения блока подстановок (S-блока, S-box).
	
	Следовательно, искомой нейронной сети необходимо уметь аппроксимировать данные операции.
	
	\item Также в основе большинства криптографических преобразований лежат неизвестные (секретные) параметры, например ключ или S-блок.
	Поэтому необходимо подобрать нейронную сеть и её параметры такие, что сеть сможет на тренировочных данных научиться !имитировать эти неизвестные параметры.
	
	\item И последняя, но не менее важная задача "--- это точность аппроксимации криптографического преобразования.
\end{enumerate}
\bigskip

\subsection{Архитектура нейронных сетей}
Сперва опишем архитектуры нейронных сетей, которые используются в нейронной криптографии.
\bigskip

В нейронной криптографии широкое применение получили следующие архитектуры нейронных сетей:
\begin{enumerate}
	\item Нейронные сети прямого распространения.
	\item Рекуррентные нейронные сети.
\end{enumerate}

\noindent \textbf{Нейронные сети прямого распространения}
\bigskip

В нейронных сетях прямого распространения (feedforward neural network) все связи направлены строго от входных нейронов к выходным.
Примерами таких сетей являются перцептрон Розенблатта, многослойный перцептрон, сети Ворда.
Особое место в нейронной криптографии занимает модель многослойного перцептрона.

\bigskip

Многослойный персептрон (MLP) "--- это способ объединения персептронов для построения классификатора для более сложных наборов
данных.
\bigskip

Многослойный персептрон включает в себя следующие типы слоев:
\begin{enumerate}
	\item Входной слой: в традиционной модели этот слой является только промежуточным между входными данными и остальной частью сети. Таким
	образом, выход из нейронов, принадлежащих к этому слою, – это просто сам входной вектор.
	
	\item Скрытый слой: этот слой направлен на введение некоторой нелинейности в модель так что MLP сможет соответствовать нелинейному
	сепарабельному набору данных. Действительно, если данные, которые должны быть изучены линейно разделимы, нет необходимости в каких-либо
	скрытых слоях. В зависимости от нелинейности и сложности модели данных, число нейронов на скрытом слое или даже количество этих слоев можно
	увеличить. Однако, один скрытый слой достаточен для большого количества естественных проблем.
	Что касается количества нейронов на скрытых слоях, было продемонстрировано, что использование огромного количества нейронов
	может привести к переобучению – достижению хороших результатов на обучающей выборке и плохих – на других данных.
	\item Выходной слой: это последний слой сети. Выходные данные узлов на этом слое непосредственно сопоставляются с классами, которые пользователь
	намеревается предсказать.
\end{enumerate}

\bigskip
\noindent \textbf{Рекуррентные нейронные сети}
\bigskip

Рекуррентные нейронные сети "--- вид нейронных сетей, где связи между элементами образуют направленную последовательность.
Благодаря этому появляется возможность обрабатывать серии событий во времени или последовательные пространственные цепочки.
В отличие от многослойных персептронов, рекуррентные сети могут использовать свою внутреннюю память для обработки
последовательностей произвольной длины. Поэтому сети RNN применимы в таких задачах, где нечто целостное разбито
на части, например: распознавание рукописного текста или распознавание речи.

\subsection{Параметры нейронных сетей}
\bigskip
Можно выделить следующие параметры нейронной сети:
\begin{enumerate}
	\item функция активации,
	\item функция потерь,
	\item количество нейронов на скрытых слоях.
\end{enumerate}
\bigskip
\noindent \textbf{Функция активации}

\bigskip
Функция активации нейрона определяет выходное значение нейрона в зависимости от результата взвешенной суммы входов и порогового значения.
Основная "функция" функции активации ввести нелинейность в выход нейрона. Если в нейронное сети будет отсутствовать функция активации, то выходной сигнал будет линейной функцией. Следовательно такая нейронная сеть не сможет изучать и моделировать (аппроксимировать) сложные данные или сложные математические модели.
\bigskip

Выделяют следующие функции активации:
\begin{enumerate}
	\item сигмоидальная функция активации: $\frac{1}{1+e^{-x}}$,
	
	\item ReLU (Rectified Linear Unit): $max(0, x)$,
	
	\item Leaky ReLU: $max(0.01x, x)$,
	
	\item тангенс (tanh): $tanh(x)$.
\end{enumerate}

\bigskip
\noindent \textbf{Функция потерь}

\bigskip
Функция потерь используется для расчета ошибки между эталонными и предсказанными выходными векторами. Основная цель нейронной сети – минимизировать эту ошибку. Таким образом, функция потерь эффективно приближает обучение нейронной сети к этой цели. Функция потерь измеряет «насколько хороша» нейронная сеть в отношении данной обучающей выборки и ожидаемых ответов.
\bigskip

Наиболее популярные функции потерь:
\begin{enumerate}
	\item квадратичная (среднеквадратичное отклонение):
	
	$MSE(w) = \frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y_i})^2$,
	\item бинарная кросс-энтропия: 
	
	$J(w) = \frac{1}{N}\sum_{n=1}^{N}H(p_n, q_n) = - \frac{1}{N}\sum_{n=1}^{n}[y_n\ log\hat{y_n} + (1 - y_n)\ log(1-\hat{y_n})]$,
	\item cреднее абсолютное отклонение:
	
	$MAE(w) = \frac{1}{N}\sum_{i=1}^{N} |y_i - \hat{y_i}|$.
\end{enumerate}

\bigskip
\subsection{Оценивание нейронных сетей и ее параметров}
\label{chapter1_section3}
Сперва, рассмотрим бинарные операции (элементы криптографических преобразований) и в подследствии определим нейронные сети аппроксимирующие их.
\bigskip

Рассмотрим $n$-мерное векторное пространство $V_n$ над полем их двух элементов $\mathbb{F}_2$. Операция сложения в поле  $\mathbb{F}_2$ обозначается символом $\oplus$. Пусть $a=(a_1, ..., a_n),\ b=(b_1,...,b_n)\in V_n$, тогда вектор $a$ будем отождествлять с числом $\bar{a} = 2^{n-1}a_1+...+2a_{n-1} + a_n$. Тогда результат операции $a \boxplus b$ есть вектор $c \in V_n$ такой, что $\bar{c}=(\bar{a} + \bar{b})\ mod\ 2^{n}$ \cite{practice}.

\begin{Lemma}
	Операцию $\boxplus$ можно представить в следующем виде:
\begin{equation}
a \boxplus b = (\bar{a} + \bar{b}) - z * 2^n,\ a, b \in V_n, \ z \in \{0, 1\}.
\label{f1}
\end{equation}

Доказательство:
\begin{equation*}
\bar{a} + \bar{b} =  2^{n} c_1 + 2^{n-1} c_2+...+2 c_{n} + c_{n+1} = \bar{c}.
\end{equation*}
\begin{equation*}
(\bar{a} + \bar{b})\ mod\ 2^{n} = 2^{n-1} c_2+...+2c_{n} + c_{n+1} = \bar{c} - 2^{n}c_1 = \bar{a} + \bar{b} - 2^{n}c_1.
\end{equation*}
\begin{equation*}
a \boxplus b = (\bar{a} + \bar{b}) - c_1 * 2^n.
\end{equation*}

\end{Lemma}

\bigskip
Очевидно, что операция $\boxplus$ "--- нелинейная операция. И исходя из этого можно сделать предположение, что однослойная нейронная сеть не сможет аппроксимировать данную операцию исходя из своих свойств и нелинейности операции $\boxplus$.
И, следовательно, из свойств многослойного перцептрона можно что сделать предположение, что нейронная сеть с такой архитектурой с легкостью предскажет (аппроксимирует) данную операцию.

\bigskip
Так как операция циклического сдвига и применение S-блока "--- это частный случай операции перестановки, то можно ввести следующие предположение.
\begin{Assumption}
	Сложность аппроксимации операции циклического сдвига и применение S-блока сравнима со сложностью аппроксимации операции перестановки.
\end{Assumption}

Основываясь на следующие исследования\cite{bnb}\cite{towards} и статью\cite{implement}, будем считать, что так как операция побитового исключения ($\oplus$) - нелинейна, то однослойная нейронная сеть не способна аппроксимировать данную операцию. Но было доказано что, нейронные сети со скрытыми слоями при достаточном количестве обучающих данных могут аппроксимировать $\oplus$ с высокой долей точности.
\bigskip

Так же в данном исследовании\cite{permutation} было показано, что нейронная сеть с многослойным персептроном способна успешно аппроксимировать операцию перестановки.
\bigskip

Основываясь на сделанных предположениях в дальнейших исследованиях мы будем использовать нейронную сеть прямого распространения с
многослойным персептроном. А для проверки наших теоретических предположений, мы построим однослойную нейронную сеть и сравним две этих модели перцептрона. Если наши теоретически предположения верны, тогда нейронная сеть с многослойным персептроном должна показать точность значительно выше однослойной сети.


